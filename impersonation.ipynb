{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"jianna_10_sample.csv\")\n",
    "\n",
    "# Combine past answers into a single string\n",
    "jianna_knowledge = \"\\n\".join([f\"Q: {row['Question']}\\nA: {row['Answer']}\" for _, row in df.iterrows()])\n",
    "# print(jianna_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('jianna_5_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy 1: zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_response = []\n",
    "def zero_shot(question):\n",
    "    prompt = f\"\"\" Answer the question.\n",
    "    Question:{question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model = 'gpt-4o',\n",
    "        input = [\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    zero_shot_response.append(response.output_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy 2: few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_response = []\n",
    "def few_shot(question, knowledge):\n",
    "    prompt = f\"\"\"You are Jianna, an international student female from South Korea. Here are some of Jianna's past answers:\n",
    "    {knowledge}\n",
    "    Based on these answers, respond in Jianna's style.\n",
    "    Question:{question}\n",
    "    Answer:\"\"\"\n",
    "       \n",
    "    response = client.responses.create(\n",
    "        model = 'gpt-4o',\n",
    "        input=[\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }]\n",
    "\n",
    "    )\n",
    "    few_shot_response.append(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy 3: RAG\n",
    "\n",
    "AI use clarification: I used Chatgpt GPT-4o to generate the first version of code by asking \"How do I use RAG to do impersonation?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# convert questions into embedding\n",
    "df[\"question_embedding\"] = df['Question'].apply(lambda x: embedding_model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def find_similar_question(question, df, model, top_k = 3):\n",
    "    question_embedding = model.encode(question)\n",
    "\n",
    "    similarities = cosine_similarity([question_embedding], np.stack(df['question_embedding'].values))[0]\n",
    "\n",
    "    top_index = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "    relevant_answers = \"\\n\".join([f\"Q: {df.iloc[i]['Question']}\\nA: {df.iloc[i]['Answer']}\" for i in top_index])\n",
    "\n",
    "    return relevant_answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_response=[]\n",
    "def rag_prompt(question, df, model):\n",
    "    relevant_answers = find_similar_question(question, df, model)\n",
    "\n",
    "    prompt = f\"\"\"You are Jianna, an international student female from South Korea. Here are some of Jianna's past answers to some related questions:\n",
    "    {relevant_answers} \n",
    "    Question: {question}    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model = 'gpt-4o',\n",
    "        input=[\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }]\n",
    "\n",
    "    )\n",
    "    \n",
    "    rag_prompt_response.append(response.output_text)\n",
    "    return relevant_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "To evaluate the impersonation performance, we can compare the similarity between the groud truth and AI-generated answer. In this article (https://huggingface.co/tasks/sentence-similarity) from HuggingFace, there are two approach to measure sentence similarity. One is Cosine Similarity and the other is Mean Reciprocal Rank. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity using Huggingface\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#Compute embedding for both lists\n",
    "def cosine_test(zero_shot_response,few_shot_response,rag_prompt_response, answer, model):\n",
    "    embedding_zero= model.encode(zero_shot_response, convert_to_tensor=True)\n",
    "    embedding_few= model.encode(few_shot_response, convert_to_tensor=True)\n",
    "    embedding_rag= model.encode(rag_prompt_response, convert_to_tensor=True)\n",
    "    embedding_answer = model.encode(answer, convert_to_tensor=True)\n",
    "    \n",
    "    cosine_zero = util.pytorch_cos_sim(embedding_zero, embedding_answer)\n",
    "    cosine_few = util.pytorch_cos_sim(embedding_few, embedding_answer)\n",
    "    cosine_rag = util.pytorch_cos_sim(embedding_rag, embedding_answer)\n",
    "\n",
    "    return cosine_zero, cosine_few, cosine_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Mean reciprocal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_token = os.getenv(\"HF_API_TOKEN\")\n",
    "\n",
    "API_URL = \"https://router.huggingface.co/hf-inference/models/sentence-transformers/msmarco-distilbert-base-tas-b\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def passage_ranking(zero_shot_response, few_shot_response, rag_prompt_response, answer):\n",
    "    data = query(\n",
    "        {\n",
    "            \"inputs\": {\n",
    "                \"source_sentence\": answer,\n",
    "                \"sentences\": [\n",
    "                    zero_shot_response,\n",
    "                    few_shot_response, \n",
    "                    rag_prompt_response,\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the three prompting & print out evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate answers via gpt API\n",
    "for i in range(5):\n",
    "    question = df_test[\"Question\"][i]\n",
    "    answer = df_test['Answer'][i]\n",
    "    \n",
    "    # Be CAREFUL, Run this takes money\n",
    "    # zero_shot(question)\n",
    "    # few_shot(question, jianna_knowledge)\n",
    "    # rag_prompt(question, df, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save answers for human judgement\n",
    "df_test_zero = df_test.copy()\n",
    "df_test_zero['Zero-shot Answer'] = zero_shot_response\n",
    "df_test_zero.to_csv('zero-shot test - answers.csv', index=False)\n",
    "\n",
    "df_test_few = df_test.copy()\n",
    "df_test_few['Few-shot Answer'] = few_shot_response\n",
    "df_test_few.to_csv('few-shot test - answers.csv', index=False)\n",
    "\n",
    "df_test_rag = df_test.copy()\n",
    "df_test_rag['RAG-shot Answer'] = rag_prompt_response\n",
    "df_test_rag.to_csv('RAG-shot test - answers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0\n",
      "(tensor([[0.5693]]), tensor([[0.4381]]), tensor([[0.3430]]))\n",
      "------\n",
      "Question 1\n",
      "(tensor([[0.7892]]), tensor([[0.8123]]), tensor([[0.6754]]))\n",
      "------\n",
      "Question 2\n",
      "(tensor([[0.6551]]), tensor([[0.6876]]), tensor([[0.6241]]))\n",
      "------\n",
      "Question 3\n",
      "(tensor([[0.4931]]), tensor([[0.4903]]), tensor([[0.5876]]))\n",
      "------\n",
      "Question 4\n",
      "(tensor([[0.4069]]), tensor([[0.3598]]), tensor([[0.4458]]))\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Question\", i)\n",
    "    answer = df_test['Answer'][i]\n",
    "    print(cosine_test(zero_shot_response[i],few_shot_response[i],rag_prompt_response[i], answer, model))\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0\n",
      "[0.8188427090644836, 0.78847736120224, 0.7755982875823975]\n",
      "------\n",
      "Question 1\n",
      "[0.8991299867630005, 0.8802077770233154, 0.8572396636009216]\n",
      "------\n",
      "Question 2\n",
      "[0.86814284324646, 0.8591591715812683, 0.8261526823043823]\n",
      "------\n",
      "Question 3\n",
      "[0.730859637260437, 0.7916536927223206, 0.8310517072677612]\n",
      "------\n",
      "Question 4\n",
      "[0.731633186340332, 0.7101984620094299, 0.7590171098709106]\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Question\", i)\n",
    "    answer = df_test['Answer'][i]\n",
    "    print(passage_ranking(zero_shot_response[i], few_shot_response[i], rag_prompt_response[i], answer))\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
