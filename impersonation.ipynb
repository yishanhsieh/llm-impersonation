{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"yishan_10_sample.csv\")\n",
    "\n",
    "# Combine past answers into a single string\n",
    "yishan_knowledge = \"\\n\".join([f\"Q: {row['Question']}\\nA: {row['Answer']}\" for _, row in df.iterrows()])\n",
    "# print(yishan_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('yishan_10_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy 1: zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot(question):\n",
    "    prompt = f\"\"\" Answer the question.\n",
    "    Question:{question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model = 'gpt-4o',\n",
    "        input = [\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy 2: few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot(question, knowledge):\n",
    "    prompt = f\"\"\"You are Yishan. Here are some of Yishan's past answers:\n",
    "    {knowledge}\n",
    "    Based on these answers, respond in Yishan's style.\n",
    "    Question:{question}\n",
    "    Answer:\"\"\"\n",
    "       \n",
    "    response = client.responses.create(\n",
    "        model = 'gpt-4o',\n",
    "        input=[\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }]\n",
    "\n",
    "    )\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy 3: RAG\n",
    "\n",
    "AI use clarification: I used Chatgpt GPT-4o to generate the first version of code by asking \"How do I use RAG to do impersonation?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# convert questions into embedding\n",
    "df['question_embedding'] = df['Question'].apply(lambda x: embedding_model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def find_similar_question(question, df, model, top_k = 3):\n",
    "    question_embedding = model.encode(question)\n",
    "\n",
    "    similarities = cosine_similarity([question_embedding], np.stack(df['question_embedding'].values))[0]\n",
    "\n",
    "    top_index = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "    relevant_answers = \"\\n\".join([f\"Q: {df.iloc[i]['Question']}\\nA: {df.iloc[i]['Answer']}\" for i in top_index])\n",
    "\n",
    "    return relevant_answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_prompt(question, df, model):\n",
    "    relevant_answers = find_similar_question(question, df, model)\n",
    "\n",
    "    prompt = f\"\"\"Answer the question as if you were Yishan.\n",
    "    Here are Yishan's answers to some related questions: {relevant_answers} \n",
    "    Question: {question}    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model = 'gpt-4o',\n",
    "        input=[\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }]\n",
    "\n",
    "    )\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = df_test[\"Question\"][0]  #take the first one\n",
    "zero_shot_response = zero_shot(question)\n",
    "few_shot_response = few_shot(question, yishan_knowledge)\n",
    "rag_prompt_response = rag_prompt(question, df, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "To evaluate the impersonation performance, we can compare the similarity between the groud truth and AI-generated answer. In this article (https://huggingface.co/tasks/sentence-similarity) from HuggingFace, there are two approach to measure sentence similarity. One is Cosine Similarity and the other is Mean Reciprocal Rank. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity using Huggingface\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#Compute embedding for both lists\n",
    "def cosine_test(zero_shot_response,few_shot_response,rag_prompt_response, answer, model):\n",
    "    embedding_zero= model.encode(zero_shot_response, convert_to_tensor=True)\n",
    "    embedding_few= model.encode(few_shot_response, convert_to_tensor=True)\n",
    "    embedding_rag= model.encode(rag_prompt_response, convert_to_tensor=True)\n",
    "    embedding_answer = model.encode(answer, convert_to_tensor=True)\n",
    "    \n",
    "    cosine_zero = util.pytorch_cos_sim(embedding_zero, embedding_answer)\n",
    "    cosine_few = util.pytorch_cos_sim(embedding_few, embedding_answer)\n",
    "    cosine_rag = util.pytorch_cos_sim(embedding_rag, embedding_answer)\n",
    "\n",
    "    return cosine_zero, cosine_few, cosine_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.5261]]), tensor([[0.5535]]), tensor([[0.5108]]))\n",
      "(tensor([[0.6047]]), tensor([[0.6117]]), tensor([[0.6585]]))\n",
      "(tensor([[0.5102]]), tensor([[0.4826]]), tensor([[0.4565]]))\n",
      "(tensor([[0.5909]]), tensor([[0.5192]]), tensor([[0.4979]]))\n",
      "(tensor([[0.2907]]), tensor([[0.4160]]), tensor([[0.5898]]))\n",
      "(tensor([[0.4999]]), tensor([[0.4993]]), tensor([[0.5112]]))\n",
      "(tensor([[0.3248]]), tensor([[0.4846]]), tensor([[0.3846]]))\n",
      "(tensor([[0.4549]]), tensor([[0.4187]]), tensor([[0.4531]]))\n",
      "(tensor([[0.5715]]), tensor([[0.4523]]), tensor([[0.4372]]))\n",
      "(tensor([[0.3877]]), tensor([[0.7234]]), tensor([[0.6635]]))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Mean reciprocal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_token = os.getenv(\"HF_API_TOKEN\")\n",
    "\n",
    "API_URL = \"https://router.huggingface.co/hf-inference/models/sentence-transformers/msmarco-distilbert-base-tas-b\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def passage_ranking(zero_shot_response, few_shot_response, rag_prompt_response, answer):\n",
    "    data = query(\n",
    "        {\n",
    "            \"inputs\": {\n",
    "                \"source_sentence\": answer,\n",
    "                \"sentences\": [\n",
    "                    zero_shot_response,\n",
    "                    few_shot_response, \n",
    "                    rag_prompt_response,\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the three prompting & print out evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you agree with the statement: \"AI will repl...</td>\n",
       "      <td>Yes, I believe so. Generative AI like Chatgpt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is your religion (if any), and what are i...</td>\n",
       "      <td>My religion is Buddhism. The core value of Bud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some of the biggest challenges you fa...</td>\n",
       "      <td>Making friends from different culture is diffi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do you make friends and build a social lif...</td>\n",
       "      <td>I did part-time jobs at a school center, and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are some underrated travel destinations t...</td>\n",
       "      <td>Some parks in Bloomington are good enough for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Do you agree with the statement: \"AI will repl...   \n",
       "1  What is your religion (if any), and what are i...   \n",
       "2  What are some of the biggest challenges you fa...   \n",
       "3  How do you make friends and build a social lif...   \n",
       "4  What are some underrated travel destinations t...   \n",
       "\n",
       "                                              Answer  \n",
       "0  Yes, I believe so. Generative AI like Chatgpt ...  \n",
       "1  My religion is Buddhism. The core value of Bud...  \n",
       "2  Making friends from different culture is diffi...  \n",
       "3  I did part-time jobs at a school center, and a...  \n",
       "4  Some parks in Bloomington are good enough for ...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.792880654335022, 0.7845485806465149, 0.8008186221122742]\n",
      "[0.8503563404083252, 0.8269603848457336, 0.8320447206497192]\n",
      "[0.8101091980934143, 0.8511186242103577, 0.8422950506210327]\n",
      "[0.8285865187644958, 0.8842434287071228, 0.8457888960838318]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    question = df_test[\"Question\"][i]\n",
    "    answer = df_test['Answer'][i]\n",
    "    zero_shot_response = zero_shot(question)\n",
    "    few_shot_response = few_shot(question, yishan_knowledge)\n",
    "    rag_prompt_response = rag_prompt(question, df, embedding_model)\n",
    "\n",
    "    # print(cosine_test(zero_shot_response,few_shot_response,rag_prompt_response, answer, model))\n",
    "    print(passage_ranking(zero_shot_response, few_shot_response, rag_prompt_response, answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
